prede=test.mat[,names(coefe)]%*%coefe
mse.test[3]=mean((Hitters$Salary[test]-prede)^2)
mse.test
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
library(randomForest)
install.packages('randomForest')
library(randomForest)
View(Hitters)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = ncol(Hitters.train)-1, importance = TRUE)
rm(list=ls())
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/PSet3")
# a.
Hitters = na.omit(Hitters) # Remove the observations for whom the salary is NA
Hitters$lgSalary = log(Hitters$Salary) # log-transform the salaries
# b.
Hitters.train = Hitters[1:200,] # training set with first 200 observations
Hitters.test = Hitters[201:nrow(Hitters),] # test set with the remaining observations.
# c-d.
set.seed(1)
lambda = 10^seq(-4, -1, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
# f. the most important predictors in the boosted model
lambda[which.min(mse.test)]
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
png(file="output/ch8q10_f.png", width=1600, height=1600, res=300)
summary(boost.Hitters)
dev.off()
# g. Apply bagging to the training set
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = ncol(Hitters.train)-1, importance = TRUE)
Hitters = na.omit(Hitters) # Remove the observations for whom the salary is NA
Hitters$lgSalary = log(Hitters$Salary) # log-transform the salaries
# b.
Hitters.train = Hitters[1:200,] # training set with first 200 observations
Hitters.test = Hitters[201:nrow(Hitters),] # test set with the remaining observations.
library(leaps)
library(ggplot2)
library(glmnet)
library(ISLR)
library(tree)
library(gbm)
library(randomForest)
Hitters = na.omit(Hitters) # Remove the observations for whom the salary is NA
Hitters$lgSalary = log(Hitters$Salary) # log-transform the salaries
# b.
Hitters.train = Hitters[1:200,] # training set with first 200 observations
Hitters.test = Hitters[201:nrow(Hitters),] # test set with the remaining observations.
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = ncol(Hitters.train)-1, importance = TRUE)
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = 19, importance = TRUE)
#ncol(Hitters.train)-1
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = 19, importance = TRUE)
#ncol(Hitters.train)-1
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
set.seed(1)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = 19, importance = TRUE)
#ncol(Hitters.train)-1
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
set.seed(1)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = 19, importance = TRUE)
#ncol(Hitters.train)-1
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
set.seed(1)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = ncol(Hitters.train)-1, importance = TRUE)
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
set.seed(1)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = ncol(Hitters.train)-1, importance = TRUE)
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
View(Hitters)
View(Hitters)
set.seed(1)
bag.Hitters = randomForest(lgSalary~., data = Hitters.train,
mtry = ncol(Hitters.train)-2, importance = TRUE)
# test MSE
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$lgSalary)^2)
cv.carseats = cv.tree(tree.carseats)
set.seed(1)
train=sample(1:nrow(Carseats), nrow(Carseats)/2)
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
# b
# Fit a regression tree to the training set.
tree.carseats = tree(Sales~., Carseats.train)
summary(tree.carseats)
# Plot the tree
png(file="output/ch8q8_b.png", width=2000, height=1000, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
# Estimate the test error
tree.pred = predict(tree.carseats, newdata=Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
# c. cross-validation
cv.carseats = cv.tree(tree.carseats)
cv.carseats
summary(tree.carseats)
tree.min <- which.min(cv.carseats$dev)
tree.min = which.min(cv.carseats$dev)
tree.min
plot(cv.carseats$size, cv.carseats$dev, type='b')
set.seed(2)
train=sample(1:nrow(Carseats), nrow(Carseats)/2)
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
tree.carseats = tree(Sales~., Carseats.train)
summary(tree.carseats)
png(file="output/ch8q8_b.png", width=2000, height=1000, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
tree.pred = predict(tree.carseats, newdata=Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
cv.carseats = cv.tree(tree.carseats)
cv.carseats
tree.min = which.min(cv.carseats$dev)
plot(cv.carseats$size, cv.carseats$dev, type='b')
set.seed(1)
train=sample(1:nrow(Carseats), nrow(Carseats)/2)
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
# b
# Fit a regression tree to the training set.
tree.carseats = tree(Sales~., Carseats.train)
summary(tree.carseats)
# Plot the tree
png(file="output/ch8q8_b.png", width=2000, height=1000, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
# Estimate the test error
tree.pred = predict(tree.carseats, newdata=Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
# c. cross-validation
cv.carseats = cv.tree(tree.carseats)
cv.carseats
tree.min = which.min(cv.carseats$dev)
plot(cv.carseats$size, cv.carseats$dev, type='b')
tree.carseats = tree(Sales~., Carseats.train)
summary(tree.carseats)
# Plot the tree
png(file="output/ch8q8_b.png", width=2000, height=1000, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
# Estimate the test error
tree.pred = predict(tree.carseats, newdata=Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
# c. cross-validation
cv.carseats = cv.tree(tree.carseats)
cv.carseats
tree.min = which.min(cv.carseats$dev)
png(file="output/ch8q8_c.png", width=2000, height=1000, res=128)
plot(cv.carseats$size, cv.carseats$dev, type='b', col='red')
dev.off()
tree.min = which.min(cv.carseats$dev)
png(file="output/ch8q8_c.png", width=2000, height=1000, res=128)
plot(cv.carseats$size, cv.carseats$dev, type='b', col='red', xlab="Tree size", ylab="CV Classification Error")
dev.off()
View(Carseats.test)
bag.carseats = randomForest(Sales~., data = Carseats.train,
mtry = ncol(Carseats.train)-1, importance = TRUE)
bag.carseats = randomForest(Sales~., data = Carseats.train,
mtry = ncol(Carseats.train)-1, importance = TRUE)
yhat.bag.carseats = predict(bag.carseats, newdata = Carseats.test)
mse.bag.carseats = mean((yhat.bag.carseats - Carseats.test$Sales)^2)
importance(bag.carseats)
m = seq(1, 10, by=1)
mse.test = rep(0, length(m))
# boosting with different shrinkage parameter.
for (i in 1:length(m)){
rf.carseats = randomForest(Sales~., data = Carseats.train,
mtry = m[i], importance = TRUE)
yhat.test.rf = predict(rf.carseats, newdata = Carseats.test)
mse.test[i] = mean((yhat.test.rf-Carseats.test$Sales)^2)
}
mse.plot = data.frame(m, mse.test)
ggplot(data = mse.plot, aes(x=m))+
geom_line(aes(y=mse.test))+
geom_point(aes(y=mse.test), shape=1)+
ylab('MSE')+
xlab('Number of variables considered at each split')
ggplot(data = mse.plot, aes(x=m))+
geom_line(aes(y=mse.test), col = 'red')+
geom_point(aes(y=mse.test), col = 'red', shape=1)+
ylab('MSE')+
xlab('Number of variables considered at each split')
ggsave("output/ch8q8_e.png", width = 6, height = 4, dpi = 300)
rf.carseats = randomForest(Sales~., data = Carseats.train,
mtry = 3, importance = TRUE)
yhat.test.rf = predict(rf.carseats, newdata = Carseats.test)
mse.test = mean((yhat.test.rf-Carseats.test$Sales)^2)
importance(rf.carseats)
View(mse.plot)
Hitters = na.omit(Hitters) # Remove the observations for whom the salary is NA
Hitters$Salary = log(Hitters$Salary) # log-transform the salaries
Hitters.train = Hitters[1:200,] # training set with first 200 observations
Hitters.test = Hitters[201:nrow(Hitters),] # test set with the remaining observations.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
set.seed(1)
lambda = 10^seq(-4, -2, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
Hitters = na.omit(Hitters) # Remove the observations for whom the salary is NA
Hitters$Salary = log(Hitters$Salary) # log-transform the salaries
# b.
train = 1:200
Hitters.train = Hitters[train,] # training set with first 200 observations
Hitters.test = Hitters[-train,] # test set with the remaining observations.
set.seed(1)
lambda = 10^seq(-4, -2, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
set.seed(1)
lambda = 10^seq(-10, -0.2, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
ggplot(data = mse.plot, aes(x=lambda), log="x")+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
scale_x_continuous(trans='log10')+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
set.seed(1)
lambda = 10^seq(-4, -0.2, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
scale_x_continuous(trans='log10')+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
set.seed(1)
lambda = 10^seq(-4, 1, by=0.2)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
scale_x_continuous(trans='log10')+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
set.seed(1)
lambda = 10^seq(-4, 0, by=0.2)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
scale_x_continuous(trans='log10')+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
lambda[which.min(mse.test)]
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
png(file="output/ch8q10_f.png", width=1600, height=1600, res=300)
summary(boost.Hitters)
dev.off()
rm(list=ls())
Hitters = na.omit(Hitters) # Remove the observations for whom the salary is NA
Hitters$Salary = log(Hitters$Salary) # log-transform the salaries
train = 1:200
Hitters.train = Hitters[train,] # training set with first 200 observations
Hitters.test = Hitters[-train,] # test set with the remaining observations.
# c-d.
set.seed(1)
lambda = 10^seq(-4, 0, by=0.2)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
scale_x_continuous(trans='log10')+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
lambda[which.min(mse.test)]
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
png(file="output/ch8q10_f.png", width=1600, height=1600, res=300)
summary(boost.Hitters)
dev.off()
set.seed(1)
bag.Hitters = randomForest(Salary~., data = Hitters.train,
mtry = ncol(Hitters.train)-1, importance = TRUE)
yhat.bag = predict(bag.Hitters, newdata = Hitters.test)
mse.test.bag = mean((yhat.bag - Hitters.test$lgSalary)^2)
mse.test.bag = mean((yhat.bag - Hitters.test$Salary)^2)
lambda[which.min(mse.test)]
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
yhat.boost = predict(boost.Hitters, newdata = Hitters.test)
mse.test.boost = mean((yhat.boost - Hitters.test$Salary)^2)
lm.Hitters = lm(Salary~., data = Hitters.train)
yhat.lm = predict(lm.Hitters, newdata = Hitters.test)
mse.test.lm = mean((yhat.lm - Hitters.test$Salary)^2)
library(glmnet)
lasso.Hitters = glmnet(Salary~., data = Hitters.train, alpha = 1)
x = model.matrix(Salary~., data = Hitters.train)
x.test = model.matrix(Salary~., data = Hitters.test)
lasso.Hitters = glmnet(x, Hitters.train$Salary, alpha = 1)
pred2 = predict(lasso.Hitters, newx = x.test)
mse.test.lasso = mean((yhat.lasso - Hitters.test$Salary)^2)
yhat.lasso = predict(lasso.Hitters, newx = x.test)
mse.test.lasso = mean((yhat.lasso - Hitters.test$Salary)^2)
