}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0,0.05), col="blue")
lines(prob_error[3,],type="l",col="red")
lines(prob_error[4,],type="l",col="purple")
lines(prob_error[5,],type="l",col="green")
lines(prob_error[6,],type="l",col="orange")
legend("topright",legend=c("1.a", "rho=0.","rho=0.2","rho=0.4","rho=0.6","rho=0.8"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0,0.05), col="blue")
lines(prob_error[2,],type="l",col="red")
lines(prob_error[3,],type="l",col="purple")
lines(prob_error[4,],type="l",col="green")
lines(prob_error[5,],type="l",col="orange")
legend("topright",legend=c("1.a", "rho=0.2","rho=0.4","rho=0.6","rho=0.8","rho=1.0"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0.01,0.05), col="blue")
lines(prob_error[2,],type="l",col="red")
lines(prob_error[3,],type="l",col="purple")
lines(prob_error[4,],type="l",col="green")
lines(prob_error[5,],type="l",col="orange")
legend("bottomleft",legend=c("1.a", "rho=0.2","rho=0.4","rho=0.6","rho=0.8","rho=1.0"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0.02,0.05), col="blue")
lines(prob_error[2,],type="l",col="red")
lines(prob_error[3,],type="l",col="purple")
lines(prob_error[4,],type="l",col="green")
lines(prob_error[5,],type="l",col="orange")
legend("bottomleft",legend=c("1.a", "rho=0.2","rho=0.4","rho=0.6","rho=0.8","rho=1.0"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
plot(density(pvalue))
test.mat=model.matrix(Salary~.,data=Hitters[test,])
mse.test=rep(NA,3)
# MSE in c
coefc=coef(summary(reg.train))[order(pvalue, decreasing=FALSE)][1:7]
predc=test.mat[,names(sort(pvalue, decreasing=FALSE)[1:7])]%*%coefc
reg.train=glm(Salary~., data=Hitters[train,])
pvalue=coef(summary(reg.train))[, 4]
pvalue=sort(pvalue, decreasing=FALSE)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
mse.test=rep(NA,3)
# MSE in c
coefc=coef(summary(reg.train))[order(pvalue, decreasing=FALSE)][1:7]
predc=test.mat[,names(sort(pvalue, decreasing=FALSE)[1:7])]%*%coefc
mse.test[1]=mean((Hitters$Salary[test]-predc)^2)
# MSE in d
coefd=coef(regfit.train.fwd, id=7)
predd=test.mat[,names(coefd)]%*%coefd
mse.test[2]=mean((Hitters$Salary[test]-predd)^2)
# MSE in e
coefe=coef(regfit.train, id=7)
prede=test.mat[,names(coefe)]%*%coefe
mse.test[3]=mean((Hitters$Salary[test]-prede)^2)
mse.test
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
x = seq(0, 1, .01)
G =  2*x*(1-x)
D = -x*log(x)-(1-x)*log(1-x)
E = 1-(x*(x > 0.5)+(1-x)*(x <= 0.5))
data = data.frame(x, G, D, E)
ggplot(data=data, aes(x=x))+
geom_line(aes(y=G, col='G'))+
geom_line(aes(y=D, col='D'))+
geom_line(aes(y=E, col='E'))+
xlab("p")+
ylab("Criterion")+
scale_color_hue("", labels = c(G="Gini index", D="Entropy", E="Classification error"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q3.png", width = 6, height = 4, dpi = 300)
library(leaps)
library(ggplot2)
library(glmnet)
library(ISLR)
library(tree)
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/PSet3")
x = seq(0, 1, .01)
G =  2*x*(1-x)
D = -x*log(x)-(1-x)*log(1-x)
E = 1-(x*(x > 0.5)+(1-x)*(x <= 0.5))
data = data.frame(x, G, D, E)
ggplot(data=data, aes(x=x))+
geom_line(aes(y=G, col='G'))+
geom_line(aes(y=D, col='D'))+
geom_line(aes(y=E, col='E'))+
xlab("p")+
ylab("Criterion")+
scale_color_hue("", labels = c(G="Gini index", D="Entropy", E="Classification error"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q3.png", width = 6, height = 4, dpi = 300)
# a. Split the data set into a training set and a test set.
set.seed(1)
train=sample(1:nrow(Carseats), 200)
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
tree.carseats=tree(Sales~., Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
png(file="output/ch8q8_b.png", width=800, height=800, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
png(file="output/ch8q8_b.png", width=1600, height=800, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
png(file="output/ch8q8_b.png", width=2000, height=800, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
png(file="output/ch8q8_b.png", width=2000, height=1000, res=128)
plot(tree.carseats)
text(tree.carseats,pretty=0)
dev.off()
tree.pred=predict(tree.carseats, Carseats.test)
tree.pred = predict(tree.carseats, Carseats.test)
mean((tree.pred-Carseats.test)^2)
View(Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
tree.pred = predict(tree.carseats, newdata=Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
summary(tree.carseats)
tree.pred = predict(tree.carseats, newdata=Carseats.test)
mean((tree.pred-Carseats.test$Sales)^2)
cv.carseats=cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type='b')
set.seed(1)
train=sample(1:nrow(OJ), 800)
OJ.train=OJ[train,]
OJ.test=OJ[-train,]
rm(list=())
rm(list=ls())
set.seed(1)
train=sample(1:nrow(OJ), 800)
OJ.train=OJ[train,]
OJ.test=OJ[-train,]
tree.OJ = tree(Purchase~., OJ.train)
summary(tree.OJ)
tree.OJ
png(file="output/ch8q9_d.png", width=2000, height=1000, res=128)
plot(tree.OJ)
text(tree.OJ, pretty=0)
dev.off()
png(file="output/ch8q9_d.png", width=2000, height=8000, res=128)
plot(tree.OJ)
text(tree.OJ, pretty=0)
dev.off()
png(file="output/ch8q9_d.png", width=2000, height=800, res=128)
plot(tree.OJ)
text(tree.OJ, pretty=0)
dev.off()
OJ.pred = predict(tree.OJ, newdata=OJ.test, type='class')
table(OJ.pred, OJ.test$Purchase)
error.test = mean(OJ.pred != OJ.test$Purchase)
cv.OJ = cv.tree(tree.OJ, FUN=prune.misclass)
cv.OJ
plot(cv.OJ$size, cv.OJ$dev, type="b")
png(file="output/ch8q9_g.png", width=2000, height=1000, res=128)
plot(cv.OJ$size, cv.OJ$dev, type="b")
dev.off()
png(file="output/ch8q9_g.png", width=2000, height=1000, res=128)
plot(cv.OJ$size, cv.OJ$dev, type="b")
xlab("h")
dev.off()
png(file="output/ch8q9_g.png", width=2000, height=1000, res=128)
plot(cv.OJ$size, cv.OJ$dev, type="b", col='red', xlab="Tree size", ylab="CV Classification Error")
dev.off()
cv.OJ = cv.tree(tree.OJ, FUN=prune.misclass)
cv.OJ
cv.OJ = cv.tree(tree.OJ, FUN=prune.misclass)
cv.OJ
prune.OJ=prune.misclass(tree.OJ, best=7)
summary(prune.OJ)
prune.OJ=prune.misclass(tree.OJ, best=7)
summary(prune.OJ)
png(file="output/ch8q9_i.png", width=2000, height=800, res=128)
plot(prune.OJ)
text(prune.OJ, pretty=0)
dev.off()
# k. Compare the test error rates
OJ.prune.pred=predict(prune.OJ, newdata=OJ.test,type="class")
table(OJ.prune.pred, OJ.test$Purchase)
error.prune.test = mean(OJ.prune.pred != OJ.test$Purchase) # test error rate
rm(list=ls())
Hitters = na.omit(Hitters)
Hitters.train = Hitters[1:200,]
Hitters.test = Hitters[201:,]
Hitters.test = Hitters[201::,]
Hitters.test = Hitters[-201,]
Hitters.test = Hitters[-1:200,]
Hitters.test = Hitters[::200,]
Hitters.test = Hitters[201:length(Hitters),]
Hitters.test = Hitters[201:nrow(Hitters),]
library(gbm)
install.packages("gbm")
library(gbm)
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = seq(0.001, 0.1, 0.005))
View(boost.Hitters)
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = 0.001)
lambda = 10^seq(-5, 1, by=0.1)
set.seed(1)
lambda = 10^seq(-5, 1, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train))+
geom_line(aes(y=mse.test))
set.seed(1)
lambda = 10^seq(-4, -1, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train))+
geom_line(aes(y=mse.test))
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_line(aes(y=mse.test, color='test'))+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'), type='b')+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'), type='b')+
geom_point()+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train', shape='o'))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train', shape='O'))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train', shape=1))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train', shape='1'))+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=4)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=4)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
summary(boost.Hitters)
set.seed(1)
lambda = 10^seq(-4, 0, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
# f. the most important predictors in the boosted model
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
summary(boost.Hitters)
set.seed(1)
lambda = 10^seq(-4, -.5, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
# f. the most important predictors in the boosted model
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
summary(boost.Hitters)
set.seed(1)
lambda = 10^seq(-4, -1, by=0.1)
mse.train = rep(0, length(lambda))
mse.test = rep(0, length(lambda))
# boosting with different shrinkage parameter.
for (i in 1:length(lambda)){
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[i])
yhat.train = predict(boost.Hitters, Hitters.train, n.trees = 1000)
mse.train[i] = mean((yhat.train-Hitters.train$Salary)^2)
yhat.test = predict(boost.Hitters, Hitters.test, n.trees = 1000)
mse.test[i] = mean((yhat.test-Hitters.test$Salary)^2)
}
# plot shrinkage values and training/test set MSE
mse.plot = data.frame(lambda, mse.train, mse.test)
ggplot(data = mse.plot, aes(x=lambda))+
geom_line(aes(y=mse.train, color='train'))+
geom_point(aes(y=mse.train, color='train'), shape=1)+
geom_line(aes(y=mse.test, color='test'))+
geom_point(aes(y=mse.test, color='test'), shape=1)+
ylab('MSE')+
xlab('Shrinkage values')+
scale_color_hue("", labels = c(train="training set MSE", test="test set MSE"))+
theme(legend.position = "bottom", legend.box = "horizontal")
ggsave("output/ch8q10_cd.png", width = 6, height = 4, dpi = 300)
# f. the most important predictors in the boosted model
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
summary(boost.Hitters)
boost.Hitters = gbm(Salary~., data = Hitters.train, distribution = 'gaussian',
n.trees = 1000, interaction.depth = 1, shrinkage = lambda[which.min(mse.test)])
png(file="output/ch8q10_f.png", width=1600, height=1600, res=300)
summary(boost.Hitters)
dev.off()
lambda[which.min(mse.test)]
