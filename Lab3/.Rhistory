prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p, 1-(1-(1-0.95)/p)**p,
main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0,0.05), col="blue")
lines(prob_error[3,],type="l",col="red",lwd=2)
lines(prob_error[4,],type="l",col="purple",lwd=2)
lines(prob_error[5,],type="l",col="green",lwd=2)
lines(prob_error[6,],type="l",col="orange",lwd=2)
legend("topright",legend=c("rho=0.","rho=0.2","rho=0.4","rho=0.6","rho=0.8","1.a"),
col=c("black","red","purple","green","orange","blue"), lty=1,lwd=2)
p1=seq(1, 10, by=1)
plot(p1, 1-(1-(1-0.95)/p1)**p1,
main="Probability of false rejection versus p (0.95, Bonferroni correction)", ylab="Type I Error", type="l", col="blue")
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0,0.05), col="blue")
lines(prob_error[3,],type="l",col="red",lwd=2)
lines(prob_error[4,],type="l",col="purple",lwd=2)
lines(prob_error[5,],type="l",col="green",lwd=2)
lines(prob_error[6,],type="l",col="orange",lwd=2)
legend("topright",legend=c("rho=0.","rho=0.2","rho=0.4","rho=0.6","rho=0.8","1.a"),
col=c("black","red","purple","green","orange","blue"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
rho_ls
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0,0.05), col="blue")
lines(prob_error[3,],type="l",col="red")
lines(prob_error[4,],type="l",col="purple")
lines(prob_error[5,],type="l",col="green")
lines(prob_error[6,],type="l",col="orange")
legend("topright",legend=c("1.a", "rho=0.","rho=0.2","rho=0.4","rho=0.6","rho=0.8"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0,0.05), col="blue")
lines(prob_error[2,],type="l",col="red")
lines(prob_error[3,],type="l",col="purple")
lines(prob_error[4,],type="l",col="green")
lines(prob_error[5,],type="l",col="orange")
legend("topright",legend=c("1.a", "rho=0.2","rho=0.4","rho=0.6","rho=0.8","rho=1.0"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0.01,0.05), col="blue")
lines(prob_error[2,],type="l",col="red")
lines(prob_error[3,],type="l",col="purple")
lines(prob_error[4,],type="l",col="green")
lines(prob_error[5,],type="l",col="orange")
legend("bottomleft",legend=c("1.a", "rho=0.2","rho=0.4","rho=0.6","rho=0.8","rho=1.0"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
N=20000
rho_ls=seq(0, 1, by=0.2)
p_ls=seq(1, 10, by=1)
prob_error=matrix(NA, nrow = length(rho_ls), ncol = length(p_ls))
i=1
for (rho in rho_ls) {
j=1
for (p in p_ls){
covmat=matrix(rep(rho, p*p), nrow=p, byrow=TRUE)
diag(covmat)=1
z=mvrnorm(n=N, mu=rep(0, p), Sigma=covmat)
c_value=qnorm(0.05/p,lower.tail = FALSE)
rej=z>c_value
rejcol=(rowSums(rej)==0)
prob_error[i,j]=1-sum(rejcol)/N
j=j+1
}
i=i+1
}
plot(p1, 1-(1-(1-0.95)/p1)**p1, main="Probability of false rejection versus p", ylab="Type I Error", type="l", xlab="p",ylim=c(0.02,0.05), col="blue")
lines(prob_error[2,],type="l",col="red")
lines(prob_error[3,],type="l",col="purple")
lines(prob_error[4,],type="l",col="green")
lines(prob_error[5,],type="l",col="orange")
legend("bottomleft",legend=c("1.a", "rho=0.2","rho=0.4","rho=0.6","rho=0.8","rho=1.0"),
col=c("blue","red","purple","green","orange","black"), lty=1,lwd=2)
plot(density(pvalue))
test.mat=model.matrix(Salary~.,data=Hitters[test,])
mse.test=rep(NA,3)
# MSE in c
coefc=coef(summary(reg.train))[order(pvalue, decreasing=FALSE)][1:7]
predc=test.mat[,names(sort(pvalue, decreasing=FALSE)[1:7])]%*%coefc
reg.train=glm(Salary~., data=Hitters[train,])
pvalue=coef(summary(reg.train))[, 4]
pvalue=sort(pvalue, decreasing=FALSE)
test.mat=model.matrix(Salary~.,data=Hitters[test,])
mse.test=rep(NA,3)
# MSE in c
coefc=coef(summary(reg.train))[order(pvalue, decreasing=FALSE)][1:7]
predc=test.mat[,names(sort(pvalue, decreasing=FALSE)[1:7])]%*%coefc
mse.test[1]=mean((Hitters$Salary[test]-predc)^2)
# MSE in d
coefd=coef(regfit.train.fwd, id=7)
predd=test.mat[,names(coefd)]%*%coefd
mse.test[2]=mean((Hitters$Salary[test]-predd)^2)
# MSE in e
coefe=coef(regfit.train, id=7)
prede=test.mat[,names(coefe)]%*%coefe
mse.test[3]=mean((Hitters$Salary[test]-prede)^2)
mse.test
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(leaps)
library(data.table)
setwd("/Users/hongjingpeng/Desktop/Machine Learning/Machine-Learning-2022W/PSet1")
data = dummy_cols(data, select_columns = "state")
# Note: Observations from New Jersey (21) and District of Columbia (1) have NAs,
# so we only have 47 dummy variables here.
## 5. Split the sample into training (80% of the data) and test (20% of the data) sets.
set.seed(1)
train = sample(seq_len(nrow(data)), size = 4*nrow(data)/5)
data.train = data[train,]
data.test = data[-train,]
rm(list=ls())
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/Lab3")
## Preparing Data ##
CovidData = read.csv("CovidData.csv")
vardes = read_excel("VariableDescription.xlsx")
## Data Analysis ##
## 1. Filter variables
oi = pull(vardes[vardes$Source == "Opportunity Insights", ], Variable) ## Opportunity Insights
oi_label = pull(vardes[vardes$Source == "Opportunity Insights", ], Description)
pmcovid = pull(vardes[vardes$Source == "PM_COVID", ], Variable) ## PM COVID
pmcovid_label = pull(vardes[vardes$Source == "PM_COVID", ], Description)
data = select(CovidData, c(county, state, deathspc, oi, pmcovid))
## 2. Descriptive statistics
stargazer(select(CovidData, c(oi)), digits = 2,
covariate.labels = c(oi_label), omit.summary.stat = c("p25", "p75"))
stargazer(select(CovidData, c(pmcovid)), digits = 2,
covariate.labels = c(pmcovid_label), omit.summary.stat = c("p25", "p75"))
## 3. Drop all observations with missing values.
data = na.omit(data)
library(fastDummies)
# install.packages("readxl")
library(readxl)
library(dplyr)
library(stargazer)
library(glmnet)
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/Lab3")
rm(list=ls())
CovidData = read.csv("CovidData.csv")
vardes = read_excel("VariableDescription.xlsx")
oi = pull(vardes[vardes$Source == "Opportunity Insights", ], Variable) ## Opportunity Insights
oi_label = pull(vardes[vardes$Source == "Opportunity Insights", ], Description)
pmcovid = pull(vardes[vardes$Source == "PM_COVID", ], Variable) ## PM COVID
pmcovid_label = pull(vardes[vardes$Source == "PM_COVID", ], Description)
data = select(CovidData, c(county, state, deathspc, oi, pmcovid))
oi = pull(vardes[vardes$Source == "Opportunity Insights", ], Variable) ## Opportunity Insights
oi_label = pull(vardes[vardes$Source == "Opportunity Insights", ], Description)
pmcovid = pull(vardes[vardes$Source == "PM_COVID", ], Variable) ## PM COVID
pmcovid_label = pull(vardes[vardes$Source == "PM_COVID", ], Description)
data = select(CovidData, c(county, state, deathspc, oi, pmcovid))
stargazer(select(CovidData, c(oi)), digits = 2,
covariate.labels = c(oi_label), omit.summary.stat = c("p25", "p75"))
stargazer(select(CovidData, c(pmcovid)), digits = 2,
covariate.labels = c(pmcovid_label), omit.summary.stat = c("p25", "p75"))
data = na.omit(data)
data = dummy_cols(data, select_columns = "state")
set.seed(1)
train = sample(seq_len(nrow(data)), size = 4*nrow(data)/5)
data.train = data[train,]
data.test = data[-train,]
data.train.fit = select(data.train, -c("county", "state", "state_Wisconsin"))
ols = lm(deathspc ~ .-1 , data = data.train.fit)
summary(ols)
alias(ols)
r2.train = summary(ols)$r.sq
mse.train = mean(ols$residuals^2)
data.test.fit = select(data.test, -c("county", "state", "state_Wisconsin"))
pred.test = predict(ols, data.test.fit)
y.test = data.test.fit$deathspc
mse.test = mean((y.test - pred.test)^2)
rss = sum((y.test - pred.test)^2)
tss = sum((y.test - mean(y.test))^2)
r2.test = 1 - rss/tss
grid = 10^seq(2, -2, length=100)
y = data.train$deathspc
x = model.matrix(deathspc~., data.train.fit)[,-1]
## Ridge
# a. model estimation
ridge.mod=glmnet(x, y, alpha=0, lambda=grid) # glmnet() standardizes the variables by default
# b. 10-fold cross-validation
ridge.cv.out = cv.glmnet(x, y, alpha=0, lambda=grid)
# c.plot
plot(ridge.cv.out)
# d. choosing the optimal value
ridge.bestlam = ridge.cv.out$lambda.min
# e. re-estimate using the optimal lambda
ridge.bestmod = glmnet(x, y, alpha=0, lambda=ridge.bestlam)
## Lasso
# a. model estimation
lasso.mod=glmnet(x, y, alpha=1, lambda=grid)
# b. 10-fold cross-validation
lasso.cv.out = cv.glmnet(x, y, alpha=1, lambda=grid)
# c.plot
plot(lasso.cv.out)
# d. choosing the optimal value
lasso.bestlam = lasso.cv.out$lambda.min
# e. re-estimate using the optimal lambda
lasso.bestmod = glmnet(x, y, alpha=1, lambda=lasso.bestlam)
## 8. training set and test set prediction errors (MSE & R^2) for Ridge and Lasso.
x.test = model.matrix(deathspc~., data.test.fit)[,-1]
## Ridge
ridge.pred = predict(ridge.bestmod, s = ridge.bestlam, newx =x.test)
mse.ridge = mean((ridge.pred - y.test)^2)
rss.ridge = sum((y.test - ridge.pred)^2)
r2.ridge = 1 - rss.ridge/tss
## Lasso
lasso.pred = predict(lasso.bestmod, s = lasso.bestlam, newx =x.test)
mse.lasso = mean((lasso.pred - y.test)^2)
rss.lasso = sum((y.test - lasso.pred)^2)
r2.lasso = 1 - rss.lasso/tss
library(fastDummies)
# install.packages("readxl")
library(readxl)
library(dplyr)
library(stargazer)
library(glmnet)
rm(list=ls())
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/Lab3")
## Preparing Data ##
CovidData = read.csv("CovidData.csv")
vardes = read_excel("VariableDescription.xlsx")
## Data Analysis ##
## 1. Filter variables
oi = pull(vardes[vardes$Source == "Opportunity Insights", ], Variable) ## Opportunity Insights
oi_label = pull(vardes[vardes$Source == "Opportunity Insights", ], Description)
pmcovid = pull(vardes[vardes$Source == "PM_COVID", ], Variable) ## PM COVID
pmcovid_label = pull(vardes[vardes$Source == "PM_COVID", ], Description)
data = select(CovidData, c(county, state, deathspc, oi, pmcovid))
## 2. Descriptive statistics
stargazer(select(CovidData, c(oi)), digits = 2,
covariate.labels = c(oi_label), omit.summary.stat = c("p25", "p75"))
stargazer(select(CovidData, c(pmcovid)), digits = 2,
covariate.labels = c(pmcovid_label), omit.summary.stat = c("p25", "p75"))
## 3. Drop all observations with missing values.
data = na.omit(data)
set.seed(1)
train = sample(seq_len(nrow(data)), size = 4*nrow(data)/5)
data.train = data[train,]
data.test = data[-train,]
data.train.fit = select(data.train, -c("county", "state"))
ols = lm(deathspc ~ .-1 , data = data.train.fit)
summary(ols)
r2.train = summary(ols)$r.sq
mse.train = mean(ols$residuals^2)
data.test.fit = select(data.test, -c("county", "state"))
pred.test = predict(ols, data.test.fit)
y.test = data.test.fit$deathspc
mse.test = mean((y.test - pred.test)^2)
rss = sum((y.test - pred.test)^2)
tss = sum((y.test - mean(y.test))^2)
r2.test = 1 - rss/tss
library(fastDummies)
# install.packages("readxl")
library(readxl)
library(dplyr)
library(stargazer)
library(glmnet)
rm(list=ls())
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/Lab3")
## Preparing Data ##
CovidData = read.csv("CovidData.csv")
vardes = read_excel("VariableDescription.xlsx")
## Data Analysis ##
## 1. Filter variables
oi = pull(vardes[vardes$Source == "Opportunity Insights", ], Variable) ## Opportunity Insights
oi_label = pull(vardes[vardes$Source == "Opportunity Insights", ], Description)
pmcovid = pull(vardes[vardes$Source == "PM_COVID", ], Variable) ## PM COVID
pmcovid_label = pull(vardes[vardes$Source == "PM_COVID", ], Description)
data = select(CovidData, c(county, state, deathspc, oi, pmcovid))
## 2. Descriptive statistics
stargazer(select(CovidData, c(oi)), digits = 2,
covariate.labels = c(oi_label), omit.summary.stat = c("p25", "p75"))
stargazer(select(CovidData, c(pmcovid)), digits = 2,
covariate.labels = c(pmcovid_label), omit.summary.stat = c("p25", "p75"))
## 3. Drop all observations with missing values.
data = na.omit(data)
## 4. Create dummies for states and the District of Columbia
data = dummy_cols(data, select_columns = "state")
# Note: Observations from New Jersey (21) and District of Columbia (1) have NAs,
# so we only have 47 dummy variables here.
## 5. Split the sample into training (80% of the data) and test (20% of the data) sets.
set.seed(1)
train = sample(seq_len(nrow(data)), size = 4*nrow(data)/5)
data.train = data[train,]
data.test = data[-train,]
## 6. OLS estimation
data.train.fit = select(data.train, -c("county", "state", "state_Wisconsin"))
ols = lm(deathspc ~ .-1 , data = data.train.fit)
summary(ols)
# MSE and R^2 in the training set
r2.train = summary(ols)$r.sq
mse.train = mean(ols$residuals^2)
# MSE in the test set
data.test.fit = select(data.test, -c("county", "state", "state_Wisconsin"))
pred.test = predict(ols, data.test.fit)
y.test = data.test.fit$deathspc
mse.test = mean((y.test - pred.test)^2)
# R^2 in the test set
rss = sum((y.test - pred.test)^2)
tss = sum((y.test - mean(y.test))^2)
r2.test = 1 - rss/tss
library(fastDummies)
# install.packages("readxl")
library(readxl)
library(dplyr)
library(stargazer)
library(glmnet)
rm(list=ls())
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/Lab3")
## Preparing Data ##
CovidData = read.csv("CovidData.csv")
vardes = read_excel("VariableDescription.xlsx")
## Data Analysis ##
## 1. Filter variables
oi = pull(vardes[vardes$Source == "Opportunity Insights", ], Variable) ## Opportunity Insights
oi_label = pull(vardes[vardes$Source == "Opportunity Insights", ], Description)
pmcovid = pull(vardes[vardes$Source == "PM_COVID", ], Variable) ## PM COVID
pmcovid_label = pull(vardes[vardes$Source == "PM_COVID", ], Description)
data = select(CovidData, c(county, state, deathspc, oi, pmcovid))
## 2. Descriptive statistics
stargazer(select(CovidData, c(oi)), digits = 2,
covariate.labels = c(oi_label), omit.summary.stat = c("p25", "p75"))
stargazer(select(CovidData, c(pmcovid)), digits = 2,
covariate.labels = c(pmcovid_label), omit.summary.stat = c("p25", "p75"))
## 3. Drop all observations with missing values.
data = na.omit(data)
## 4. Create dummies for states and the District of Columbia
data = dummy_cols(data, select_columns = "state")
# Note: Observations from New Jersey (21) and District of Columbia (1) have NAs,
# so we only have 47 dummy variables here.
## 5. Split the sample into training (80% of the data) and test (20% of the data) sets.
set.seed(1)
train = sample(seq_len(nrow(data)), size = 4*nrow(data)/5)
data.train = data[train,]
data.test = data[-train,]
## 6. OLS estimation
data.train.fit = select(data.train, -c("county", "state", "state_Wisconsin"))
ols = lm(deathspc ~ .-1 , data = data.train.fit)
summary(ols)
# MSE and R^2 in the training set
r2.train = summary(ols)$r.sq
mse.train = mean(ols$residuals^2)
# MSE in the test set
data.test.fit = select(data.test, -c("county", "state", "state_Wisconsin"))
pred.test = predict(ols, data.test.fit)
y.test = data.test.fit$deathspc
mse.test = mean((y.test - pred.test)^2)
# R^2 in the test set
rss = sum((y.test - pred.test)^2)
tss = sum((y.test - mean(y.test))^2)
r2.test = 1
r2.test = 1 - rss/tss
grid = 10^seq(2, -2, length=100)
y = data.train$deathspc
x = model.matrix(deathspc~., data.train.fit)[,-1]
## Ridge
# a. model estimation
ridge.mod=glmnet(x, y, alpha=0, lambda=grid) # glmnet() standardizes the variables by default
# b. 10-fold cross-validation
ridge.cv.out = cv.glmnet(x, y, alpha=0, lambda=grid)
# c.plot
plot(ridge.cv.out)
# d. choosing the optimal value
ridge.bestlam = ridge.cv.out$lambda.min
# e. re-estimate using the optimal lambda
ridge.bestmod = glmnet(x, y, alpha=0, lambda=ridge.bestlam)
ridge.mod=glmnet(x, y, alpha=0, lambda=grid) # glmnet() standardizes the variables by default
# b. 10-fold cross-validation
ridge.cv.out = cv.glmnet(x, y, alpha=0, lambda=grid)
# c.plot
png(file="output/ridge_cv.png", width=1600, height=800, res=128)
plot(ridge.cv.out)
dev.off()
ridge.bestlam = ridge.cv.out$lambda.min
# e. re-estimate using the optimal lambda
ridge.bestmod = glmnet(x, y, alpha=0, lambda=ridge.bestlam)
lasso.mod=glmnet(x, y, alpha=1, lambda=grid)
# b. 10-fold cross-validation
lasso.cv.out = cv.glmnet(x, y, alpha=1, lambda=grid)
# c.plot
png(file="output/lasso_cv.png", width=1600, height=800, res=128)
plot(lasso.cv.out)
dev.off()
# d. choosing the optimal value
lasso.bestlam = lasso.cv.out$lambda.min
# e. re-estimate using the optimal lambda
lasso.bestmod = glmnet(x, y, alpha=1, lambda=lasso.bestlam)
summary(lasso.bestmod)$r.sq
lasso.bestmod$r.sq
ridge.pred.train = predict(ridge.bestmod, s = ridge.bestlam, newx =x)
y.train = data.train.fit$deathspc
ridge.pred.train = predict(ridge.bestmod, s = ridge.bestlam, newx =x)
mse.train.ridge = mean((ridge.pred.train - y.train)^2)
rss.train.ridge = sum((y.train - ridge.pred.train)^2)
tss.train = sum((y.train - mean(y.train))^2)
r2.train.ridge = 1 - rss.train.ridge/tss.train
x = model.matrix(deathspc~., data.train.fit)[,-1]
ridge.pred.train = predict(ridge.bestmod, s = ridge.bestlam, newx =x)
mse.train.ridge = mean((ridge.pred.train - y.train)^2)
rss.train.ridge = sum((y.train - ridge.pred.train)^2)
tss.train = sum((y.train - mean(y.train))^2)
r2.train.ridge = 1 - rss.train.ridge/tss.train
ridge.pred = predict(ridge.bestmod, s = ridge.bestlam, newx =x.test)
mse.test.ridge = mean((ridge.pred - y.test)^2)
rss.test.ridge = sum((y.test - ridge.pred)^2)
r2.test.ridge = 1 - rss.ridge/tss
ridge.pred = predict(ridge.bestmod, s = ridge.bestlam, newx =x.test)
mse.test.ridge = mean((ridge.pred - y.test)^2)
rss.test.ridge = sum((y.test - ridge.pred)^2)
r2.test.ridge = 1 - rss.test.ridge/tss
ridge.pred = predict(ridge.bestmod, s = ridge.bestlam, newx =x.test)
mse.test.ridge = mean((ridge.pred - y.test)^2)
rss.test.ridge = sum((y.test - ridge.pred)^2)
r2.test.ridge = 1 - rss.test.ridge/tss
x.test = model.matrix(deathspc~., data.test.fit)[,-1]
ridge.pred = predict(ridge.bestmod, s = ridge.bestlam, newx =x.test)
mse.test.ridge = mean((ridge.pred - y.test)^2)
rss.test.ridge = sum((y.test - ridge.pred)^2)
r2.test.ridge = 1 - rss.test.ridge/tss
lasso.pred.train = predict(lasso.bestmod, s = lasso.bestlam, newx =x)
mse.train.lasso = mean((lasso.pred.train - y.train)^2)
rss.train.lasso = sum((y.train - lasso.pred.train)^2)
r2.train.lasso = 1 - rss.train.lasso/tss.train
lasso.pred = predict(lasso.bestmod, s = lasso.bestlam, newx =x.test)
mse.lasso = mean((lasso.pred - y.test)^2)
rss.lasso = sum((y.test - lasso.pred)^2)
r2.lasso = 1 - rss.lasso/tss
r2.test.lasso = 1 - rss.lasso/tss
mse.test.lasso = mean((lasso.pred - y.test)^2)
