---
title: "PSet 4"
author: "Jingpeng Hong"
date: "3/5/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Housekeeping, include=FALSE}
# install.packages("tm")
library(tm)
library(dplyr)
# install.packages("SnowballC")
library(SnowballC)
# install.packages("wordcloud")
library(wordcloud)
# install.packages("topicmodels")
library(topicmodels)
# install.packages("tidytext")
library(tidytext)
# install.packages("reshape2")
library(reshape2)

rm(list=ls())
setwd("/Users/hongjingpeng/Desktop/Machine\ Learning/Machine-Learning-2022W/PSet4")
```

**Question 1**

**a.** 

Load the data as a corpus.
```{r Q1_a}
texts = file.path("SimpleText_auto") 
docs_raw = VCorpus(DirSource(texts))
```

**b.** 

Clean the data.
```{r Q1_b}
docs = docs_raw %>%
  tm_map(content_transformer(tolower)) %>% # transform all characters to lowercase
  tm_map(removeWords, stopwords("english")) %>% # remove stop words
  tm_map(removeWords, c('table', 'figure', 'results', 'use', 'can', 'also')) %>% 
  tm_map(removePunctuation) %>% # remove punctuation
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) %>% # remove excess whitespace
  tm_map(stemDocument) # get to words' roots
```
Justify our answers.
```{r Q1_b justification}
docs_raw[[1]]$content[4]
docs[[1]]$content[4]
```
**c.** 

Present the 50 most frequently used words in the corpus in an Word Cloud.

```{r Q1_c}
wordcloud(docs, max.words = 50, scale=c(2, .2))
```

**d.**

Fit a topic model on the corpus setting k equal to 2, 3, 5, 8, and 10. 
```{r Q1_d}
set.seed(123)
dtm = DocumentTermMatrix(docs)

# define the function of topic models with k.
topic = function(k){
  lda = LDA(dtm, k = k, method = "Gibbs", control = list(burnin = 100, iter = 1000))
  topics = tidy(lda, matrix = "beta") 
  topwords = topics %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta) # print the words with the highest beta from each topic
  for (i in 1:k) {
    print(topwords %>% filter(topic==i))
  }
}
```

```{r k=2}
topic(2)
```
The first topic is hard to be defined, while the second topic is like Biology/Ecology.
```{r k=3}
topic(3)
```
The first topic is similar to physics, with specific issues related to Particle physics/Nuclear physics, because we have words energy, electron, flow temperature; The second topic is ecology or biology physics, with words cell, soil, plant, gene; The third topic is ambiguous but looks like computer science, mathematics or statistics.
```{r k=5}
topic(5)
```

```{r k=8}
topic(8)
```

```{r k=10}
topic(10)
```

**e.**

Optimize the hyperparameters of the LDA model using 10-fold cross-validation.


